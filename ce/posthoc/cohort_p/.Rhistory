if (i > 1) {
len.i$prefix <- substr(len.i$phones, 1, i-1)
len.i$prefix.freq <- cohorts$freq[match(len.i$prefix, cohorts$prefix)]
len.i$phone.freq <- cohorts$freq[match(len.i$prefix.plus.seg, cohorts$prefix)]
} else {
len.i$prefix.freq <- total_freq
len.i$phone.freq <- cohorts$freq[match(len.i$phone, cohorts$prefix)]
}
len.i
})
si <- ldply(si, data.frame)
si$cohort.size <- cohorts$cohort.count[match(si$prefix.plus.seg, cohorts$prefix)]
si$prefix.cohort.size <- cohorts$cohort.count[match(si$prefix, cohorts$prefix)]
si$prefix.cohort.size[si$position == 1] <- total_count
si$seg.p.type <- si$cohort.size / si$prefix.cohort.size
si$seg.p <- si$phone.freq / si$prefix.freq
si$seg.p.excluded <- (si$phone.freq - si$freq) / (si$prefix.freq - si$freq)
si$seg.p.excluded[si$cohort.size == 1] <-
si$seg.p[si$cohort.size == 1]
si <- subset(si, select =-c(prefix, prefix.plus.seg, prefix.cohort.size))
si$seg.h <- -log2(si$seg.p)
si$seg.h.type <- -log2(si$seg.p.type)
si
}
entropy_by_cohort <- function(d, include_post_up = F, min_per_cohort = 1, min_segs = 1) {
if (!include_post_up) {
d$phones <- substr(d$phones, 1, d$up)
}
total_freq <- sum(d$freq)
cohorts <- build_cohorts(d)
cohorts$cohort.p <- cohorts$last.freq / total_freq
cohorts$seg.p <- cohorts$freq / cohorts$last.freq
cohorts.H <- aggregate(seg.p ~ last.one + cohort.p, cohorts, filtered_H)
names(cohorts.H)[3] <- 'entropy'
cohorts.nb_segs <- aggregate(seg.p ~ last.one, cohorts, filtered_H_length)
names(cohorts.nb_segs)[2] <- 'nb.cont.segs'
cohorts.cohort_count <- aggregate(cohort.count ~ last.one, cohorts, sum)
chrt <- join(cohorts.H, cohorts.nb_segs, by = 'last.one')
chrt <- join(chrt, cohorts.cohort_count, by = 'last.one')
names(chrt)[1] <- 'cohort'
chrt$position <- nchar(chrt$cohort) + 1
chrt$position[chrt$cohort == 0] <- 1
chrt$cohort.h <- -log2(chrt$cohort.p)
chrt$log.prob <- -chrt$cohort.h
###
chrt <- subset(chrt, cohort.count >= min_per_cohort & nb.cont.segs >= min_segs)
chrt$log.cont.segs <- log2(chrt$nb.cont.segs)
chrt$log.cohort.count <- log2(chrt$cohort.count)
###
chrt$cohort.h.by_length <- 0
chrt$nb.cont.segs.by_length <- 0
chrt$log.cont.segs.by_length <- 0
chrt$entropy.by_length <- 0
chrt$cohort.count.by_length <- 0
chrt$log.cohort.count.by_length <- 0
for (i in 1:max(chrt$position)) {
chrt$cohort.h.by_length[chrt$position == i] <- scale(chrt$cohort.h[chrt$position == i])
chrt$nb.cont.segs.by_length[chrt$position == i] <- scale(chrt$nb.cont.segs[chrt$position == i])
chrt$log.cont.segs.by_length[chrt$position == i] <- scale(chrt$log.cont.segs[chrt$position == i])
chrt$entropy.by_length[chrt$position == i] <- scale(chrt$entropy[chrt$position == i])
chrt$cohort.count.by_length[chrt$position == i] <- scale(chrt$cohort.count[chrt$position == i])
chrt$log.cohort.count.by_length[chrt$position == i] <- scale(chrt$log.cohort.count[chrt$position == i])
}
chrt[is.na(chrt)] <- 0
chrt$log.prob.by_length <- -chrt$cohort.h.by_length
chrt$entropy.by_segs <- 0
chrt$log.prob.by_segs <- 0
for (i in 1:max(chrt$nb.cont.segs)) {
chrt$entropy.by_segs[chrt$nb.cont.segs == i] <-
scale(chrt$entropy[chrt$nb.cont.segs == i])
chrt$log.prob.by_segs[chrt$nb.cont.segs == i] <-
scale(chrt$log.prob[chrt$nb.cont.segs == i])
}
chrt
}
#####
d <- lf('~/rdy/lexicons_ce/English.lex.txt')
chrt <- entropy_by_cohort(d)
scrs <- pblapply(1:5, function(i) {
d.scr <- shuffle_frequencies(d, shuffle_same_tail = T)
chrt.scr <- entropy_by_cohort(d.scr)
})
#####
d <- lf('~/rdy/lexicons_ce/English.lex.txt')
chrt <- entropy_by_cohort(d)
print(sum(chrt$cohort.p * chrt$entropy))
scrs <- pblapply(1:5, function(i) {
d.scr <- shuffle_frequencies(d, shuffle_same_tail = T)
chrt.scr <- entropy_by_cohort(d.scr)
print(sum(chrt.scr$cohort.p * chrt.scr$entropy))
})
d.scr <- shuffle_frequencies(d, shuffle_same_tail = F)
scrs <- pblapply(1:5, function(i) {
d.scr <- shuffle_frequencies(d, shuffle_same_tail = F)
chrt.scr <- entropy_by_cohort(d.scr)
print(sum(chrt.scr$cohort.p * chrt.scr$entropy))
})
scrs <- pblapply(1:5, function(i) {
d.scr <- shuffle_frequencies(d, shuffle_same_tail = T)
chrt.scr <- entropy_by_cohort(d.scr)
print(sum(chrt.scr$cohort.p * chrt.scr$entropy))
})
chrt <- entropy_by_cohort(d, include_post_up = T)
print(sum(chrt$cohort.p * chrt$entropy))
scrs <- pblapply(1:5, function(i) {
d.scr <- shuffle_frequencies(d, shuffle_same_tail = T)
chrt.scr <- entropy_by_cohort(d.scr, include_post_up = T)
print(sum(chrt.scr$cohort.p * chrt.scr$entropy))
})
#####
d <- lf('~/rdy/lexicons_ce/English.lex.txt')
chrt <- entropy_by_cohort(d, include_post_up = T)
chrt$exp.h <- chrt$cohort.p * chrt$entropy
chrt.scr$exp.h <- chrt$cohort.p * chrt.scr$entropy
chrt.scr$exp.h <- chrt.scr$cohort.p * chrt.scr$entropy
chrt.scr <- entropy_by_cohort(d.scr, include_post_up = T)
#####
d <- lf('~/rdy/lexicons_ce/English.lex.txt')
chrt <- entropy_by_cohort(d, include_post_up = F)
d.scr <- shuffle_frequencies(d, shuffle_same_tail = T)
chrt.scr <- entropy_by_cohort(d.scr, include_post_up = F)
print(sum(chrt$cohort.p * chrt$entropy))
print(sum(chrt.scr$cohort.p * chrt.scr$entropy))
chrt$exp.h <- chrt$cohort.p * chrt$entropy
chrt.scr$exp.h <- chrt.scr$cohort.p * chrt.scr$entropy
ggplot(data = chrt, aes(exp.h)) + geom_histogram()
ggplot(data = chrt.nonce, aes(exp.h)) + geom_histogram()
ggplot(data = chrt.scr, aes(exp.h)) + geom_histogram()
View(chrt)
View(chrt.scr)
chrt <- subset(chrt, entropy > 0)
chrt.scr <- subset(chrt.scr, entropy > 0)
print(sum(chrt$cohort.p * chrt$entropy))
print(sum(chrt.scr$cohort.p * chrt.scr$entropy))
chrt$exp.h <- chrt$cohort.p * chrt$entropy
chrt.scr$exp.h <- chrt.scr$cohort.p * chrt.scr$entropy
ggplot(data = chrt, aes(exp.h)) + geom_histogram()
ggplot(data = chrt.scr, aes(exp.h)) + geom_histogram()
View(chrt)
ggplot(data = subset(chrt, exp.h < 0), aes(exp.h)) + geom_histogram()
ggplot(data = subset(chrt, exp.h < 1), aes(exp.h)) + geom_histogram()
ggplot(data = subset(chrt.scr, exp.h < 1), aes(exp.h)) + geom_histogram()
ggplot(data = subset(chrt, exp.h < .5), aes(exp.h)) + geom_histogram()
ggplot(data = subset(chrt.scr, exp.h < .5), aes(exp.h)) + geom_histogram()
View(chrt)
ggplot(data = subset(chrt, exp.h < .01), aes(exp.h)) + geom_histogram()
ggplot(data = subset(chrt.scr, exp.h < .01), aes(exp.h)) + geom_histogram()
ggplot(data = subset(chrt, exp.h < .001), aes(exp.h)) + geom_histogram()
ggplot(data = subset(chrt.scr, exp.h < .001), aes(exp.h)) + geom_histogram()
sum(chrt$entropy)
sum(chrt.scr$entropy)
sum(chrt.scr$exp.h)
sum(chrt$exp.h)
ggplot(data = subset(chrt, exp.h < .0001), aes(exp.h)) + geom_histogram()
ggplot(data = subset(chrt.scr, exp.h < .0001), aes(exp.h)) + geom_histogram()
summary(chrt$exp.h)
summary(chrt.scr$exp.h)
sum(chrt$exp.h[chrt$position > 1])
sum(chrt.scr$exp.h[chrt$position > 1])
sum(chrt.scr$exp.h[chrt.scr$position > 1])
chrt <- subset(chrt, position > 1)
library(parallel)
library(pbapply)
library(tidyverse)
library(plyr)
library(lme4)
library(lmerTest)
library(car)
library(mgcv)
library(xtable)
library(ggrepel)
library(ggplot2)
library(grid)
library(gridExtra)
library(ggpubr)
rm(list = ls())
nb_cores <- 1
nb_scramble <- 64
z. <- scale
fdir <- '~/rdy/'
lexdir <- paste0(fdir, 'lexicons_ce/')
noncedir <- paste0(fdir, 'new_forms/')
outdir <- paste0(fdir, 'ce/')
lex.files <- list.files(lexdir)
lex.files <- subset(lex.files, grepl('lex.txt$', lex.files))
filtered_H <- function(P, p_threshold = .95) {
P <- sort(P, decreasing = T)
for (i in 1:length(P)) {
if (sum(P[1:i]) >= p_threshold) {
break
}
}
P <- P[1:i]
P <- P / sum(P)
sum(P * -log2(P))
}
filtered_H_length <- function(P, p_threshold = .95) {
P <- sort(P, decreasing = T)
for (i in 1:length(P)) {
if (sum(P[1:i]) >= p_threshold) {
break
}
}
P <- P[1:i]
i
}
lmer_to_latex <- function(m) {
df <- as.data.frame(summary(m)$coefficients)
# add p-vals
if ('df' %in% names(df)) {
df <- subset(df, select = -df)
}
fixed.effects <- print(xtable(df, digits = 3))
# add extra columns to table
# add title for "fixed effects
fixed.effects <- sub('\\n  ', '\n {\\\\bf A. Fixed Effects:} \\\\\\\\\n', fixed.effects, perl = T)
fixed.effects <- gsub('\\n\\\\end.tab.*', '', fixed.effects, perl = T)
fixed.effects <- substr(fixed.effects, 1, nchar(fixed.effects)-10)
fixed.effects
random.effects <- c('\\hline \\hline',
'{\\bf B. Random Effects:} \\\\',
'\\hline',
'& Name & Variance & Std.Dev. \\\\',
'\\hline')
random.factors <- sapply(1:length(ranef(m)), function(i) {
factor.name <- names(ranef(m))[i]
factor.df <- as.data.frame(ranef(m)[i])
names(factor.df) <- sapply(names(factor.df), function(s) { substr(s, nchar(factor.name)+2, nchar(s))})
intercept.var <- substr(var(factor.df[,1]),1,5)
intercept.sd <- substr(sd(factor.df[,1]),1,5)
factor_str <- paste0(c(factor.name, ' & (Intercept) & ', intercept.var,
' & ', intercept.sd, ' \\\\'), collapse = '')
if (length(names(factor.df))>1) {
for (j in 2:length(names(factor.df))) {
slope.name <- names(factor.df)[j]
slope.var <- substr(var(factor.df[,j]),1,5)
slope.sd <- substr(sd(factor.df[,j]),1,5)
factor_str <- paste0(c(factor_str, '\n & ', slope.name, ' & ', slope.var,
' & ', slope.sd, ' \\\\'), collapse = '')
}
}
factor_str
})
random.effects <- c(random.effects, random.factors)
latex.table <- paste0(c(fixed.effects, random.effects,
'\\end{tabular}\n\\end{table}'), collapse =  '\n')
latex.table <- gsub('family', 'Family', latex.table)
latex.table <- gsub('language', 'Language', latex.table)
latex.table <- gsub('000', '001', latex.table)
# comment out some stuff
latex.table <- gsub('\\\\begin.table', '%\\begin{table', latex.table)
latex.table <- gsub('\\\\centering', '%\\centering', latex.table)
latex.table <- gsub('\\\\end.table', '%\\end{table', latex.table)
latex.table
}
find_up <- function(d) {
phones <- sort(d$phones,method = 'radix')
ups <- sapply(1:(length(phones)), function(i) {
w <- phones[i]
for (j in nchar(w):0) {
if (i < length(phones) && substr(w,1,j) == substr(phones[i+1],1,j)) {
j <- j + 1
break
} else if (i > 1 && substr(w,1,j) == substr(phones[i-1],1,j)) {
j <- j + 1
break
}
}
if (j == 0) {
j <- nchar(w) + 1
}
j
})
df <- data.frame(phones = phones, up = ups)
d$up <- df$up[match(d$phones, df$phones)]
d
}
shuffle_frequencies <- function(d, shuffle_same_tail = F, shuffle_same_nbrhood = F, keep_length = T) {
total_freq <- sum(d$freq)
if (keep_length & shuffle_same_tail) {
for (i in 1:max(d$word.length)) {
if (nrow(subset(d, word.length == i)) > 1) {
for (j in 1:i) {
if (nrow(subset(d, word.length == i & up == j)) > 1) {
d$freq[d$word.length == i & d$up == j] <-
sample(d$freq[d$word.length == i & d$up == j])
}
}
}
}
} else if (keep_length & shuffle_same_nbrhood) {
for (i in 1:max(d$word.length)) {
if (nrow(subset(d, word.length == i)) > 1) {
for (j in 0:max(subset(d, word.length == i)$min.pairs)) {
if (nrow(subset(d, word.length == i & min.pairs == j)) > 1) {
d$freq[d$word.length == i & d$min.pairs == j] <-
sample(d$freq[d$word.length == i & d$min.pairs == j])
}
}
}
}
} else if (keep_length) {
for (i in 1:max(d$word.length)) {
if (nrow(subset(d, word.length == i)) > 1) {
d$freq[d$word.length == i] <- sample(d$freq[d$word.length == i])
}
}
} else {
d$freq <- sample(d$freq)
}
d$unig.p <- d$freq / total_freq
d$unig.h <- -log2(d$unig.p)
d$log.prob <- -d$unig.h
d
}
lf <- function(f, do_up = T, min_freq_rank = 10000) {
d <- read.csv(f, sep = '\t', header = F)
names(d) <- c('word', 'phones', 'freq')
d$word <- as.character(d$word)
d$phones <- as.character(d$phones)
d$phones <- gsub(' ', '', d$phones)
# for tone languages, exclude tone marking from length
if (grepl('Vietnamese|Cantonese', f)) {
d$word.length <- nchar(gsub('[0-9]', '', d$phones))
} else {
d$word.length <- nchar(d$phones)
}
d <- subset(d, word.length <= 12)
d <- subset(d, !(nchar(word) == 1 & nchar(phones) > 1))
d$freq <- as.double(d$freq)
d$freq.rank <- rank(-d$freq, ties.method = 'random')
if (min_freq_rank > 0) {
d <- subset(d, freq.rank <= min_freq_rank)
}
total_freq <- sum(d$freq)
d$unig.p <- d$freq / total_freq
d$unig.h <- -log2(d$unig.p)
d$log.prob <- -d$unig.h
if (do_up) {
d <- find_up(d)
}
d
}
add_language_family <- function(d) {
d$language <- as.character(d$language)
d$family <- d$language
d$family[d$language %in% c('Armenian', 'Bengali', 'Dutch',
'English', 'French', 'German', 'Italian',
'Russian', 'Slovak', 'Spanish')] <- 'Indo-European'
d$family[d$language %in% c('Arabic', 'Hausa', 'Hebrew')] <- 'Afroasiatic'
d$family[d$language %in% c('Malay', 'Tagalog')] <- 'Austronesean'
d$family[d$language %in% c('Finnish', 'Hungarian')] <- 'Uralic'
d$family[d$language %in% c('Cantonese', 'Vietnamese')] <- 'Sino-Tibetan'
d$language <- as.factor(d$language)
d
}
build_cohorts <- function(d, shuffle = F) {
total_freq <- sum(d$freq)
total_count <- length(d$freq)
if (shuffle) {
d <- shuffle_frequencies(d, F, F)
}
cohorts <- lapply(1:max(nchar(d$phones)), function(i) {
len.i <- subset(d, nchar(phones) >= i)
len.i$prefix <- substr(len.i$phones,1,i)
len.i.aggr <- aggregate(freq ~ prefix, len.i, FUN = sum)
len.i.aggr.counts <- aggregate(freq ~ prefix, len.i, FUN = length)
names(len.i.aggr.counts)[2] <- 'cohort.count'
len.i.aggr <- left_join(len.i.aggr, len.i.aggr.counts, by = 'prefix')
len.i.aggr$cohort.count.scaled <- scale(len.i.aggr$cohort.count)
len.i.aggr
})
cohorts <- ldply(cohorts, data.frame)
cohorts$last.one <- substr(cohorts$prefix, 1, nchar(cohorts$prefix) - 1)
cohorts$last.one[nchar(cohorts$prefix) == 1] <- '0'
cohorts$last.freq <- cohorts$freq[match(cohorts$last.one, cohorts$prefix)]
cohorts$last.freq[nchar(cohorts$prefix) == 1] <- total_freq
cohorts$last.count <- cohorts$cohort.count[match(cohorts$last.one, cohorts$prefix)]
cohorts$last.count[nchar(cohorts$prefix) == 1] <- total_count
cohorts
}
lex2si <- function(d, shuffle = F, shuffle_same_tail = F, shuffle_same_nbrhood = F) {
total_freq <- sum(d$freq)
total_count <- length(d$freq)
if (shuffle) {
d <- shuffle_frequencies(shuffle_same_tail = shuffle_same_tail,
shuffle_same_nbrhood = shuffle_same_nbrhood)
}
cohorts <- build_cohorts(d, shuffle = F)
si <- lapply(1:max(nchar(d$phones)), function(i) {
len.i <- subset(d, nchar(phones) >= i)
len.i$position <- i
len.i$phone <- substr(len.i$phones, i, i)
len.i$prefix.plus.seg <- substr(len.i$phones, 1, i)
if (i > 1) {
len.i$prefix <- substr(len.i$phones, 1, i-1)
len.i$prefix.freq <- cohorts$freq[match(len.i$prefix, cohorts$prefix)]
len.i$phone.freq <- cohorts$freq[match(len.i$prefix.plus.seg, cohorts$prefix)]
} else {
len.i$prefix.freq <- total_freq
len.i$phone.freq <- cohorts$freq[match(len.i$phone, cohorts$prefix)]
}
len.i
})
si <- ldply(si, data.frame)
si$cohort.size <- cohorts$cohort.count[match(si$prefix.plus.seg, cohorts$prefix)]
si$prefix.cohort.size <- cohorts$cohort.count[match(si$prefix, cohorts$prefix)]
si$prefix.cohort.size[si$position == 1] <- total_count
si$seg.p.type <- si$cohort.size / si$prefix.cohort.size
si$seg.p <- si$phone.freq / si$prefix.freq
si$seg.p.excluded <- (si$phone.freq - si$freq) / (si$prefix.freq - si$freq)
si$seg.p.excluded[si$cohort.size == 1] <-
si$seg.p[si$cohort.size == 1]
si <- subset(si, select =-c(prefix, prefix.plus.seg, prefix.cohort.size))
si$seg.h <- -log2(si$seg.p)
si$seg.h.type <- -log2(si$seg.p.type)
si
}
entropy_by_cohort <- function(d, include_post_up = F, min_per_cohort = 1, min_segs = 1) {
if (!include_post_up) {
d$phones <- substr(d$phones, 1, d$up)
}
total_freq <- sum(d$freq)
cohorts <- build_cohorts(d)
cohorts$cohort.p <- cohorts$last.freq / total_freq
cohorts$seg.p <- cohorts$freq / cohorts$last.freq
cohorts.H <- aggregate(seg.p ~ last.one + cohort.p, cohorts, filtered_H)
names(cohorts.H)[3] <- 'entropy'
cohorts.nb_segs <- aggregate(seg.p ~ last.one, cohorts, filtered_H_length)
names(cohorts.nb_segs)[2] <- 'nb.cont.segs'
cohorts.cohort_count <- aggregate(cohort.count ~ last.one, cohorts, sum)
chrt <- join(cohorts.H, cohorts.nb_segs, by = 'last.one')
chrt <- join(chrt, cohorts.cohort_count, by = 'last.one')
names(chrt)[1] <- 'cohort'
chrt$position <- nchar(chrt$cohort) + 1
chrt$position[chrt$cohort == 0] <- 1
chrt$cohort.h <- -log2(chrt$cohort.p)
chrt$log.prob <- -chrt$cohort.h
###
chrt <- subset(chrt, cohort.count >= min_per_cohort & nb.cont.segs >= min_segs)
chrt$log.cont.segs <- log2(chrt$nb.cont.segs)
chrt$log.cohort.count <- log2(chrt$cohort.count)
###
chrt$cohort.h.by_length <- 0
chrt$nb.cont.segs.by_length <- 0
chrt$log.cont.segs.by_length <- 0
chrt$entropy.by_length <- 0
chrt$cohort.count.by_length <- 0
chrt$log.cohort.count.by_length <- 0
for (i in 1:max(chrt$position)) {
chrt$cohort.h.by_length[chrt$position == i] <- scale(chrt$cohort.h[chrt$position == i])
chrt$nb.cont.segs.by_length[chrt$position == i] <- scale(chrt$nb.cont.segs[chrt$position == i])
chrt$log.cont.segs.by_length[chrt$position == i] <- scale(chrt$log.cont.segs[chrt$position == i])
chrt$entropy.by_length[chrt$position == i] <- scale(chrt$entropy[chrt$position == i])
chrt$cohort.count.by_length[chrt$position == i] <- scale(chrt$cohort.count[chrt$position == i])
chrt$log.cohort.count.by_length[chrt$position == i] <- scale(chrt$log.cohort.count[chrt$position == i])
}
chrt[is.na(chrt)] <- 0
chrt$log.prob.by_length <- -chrt$cohort.h.by_length
chrt$entropy.by_segs <- 0
chrt$log.prob.by_segs <- 0
for (i in 1:max(chrt$nb.cont.segs)) {
chrt$entropy.by_segs[chrt$nb.cont.segs == i] <-
scale(chrt$entropy[chrt$nb.cont.segs == i])
chrt$log.prob.by_segs[chrt$nb.cont.segs == i] <-
scale(chrt$log.prob[chrt$nb.cont.segs == i])
}
chrt
}
#####
d <- lf('~/rdy/lexicons_ce/English.lex.txt')
chrt <- entropy_by_cohort(d, include_post_up = F)
chrt <- subset(chrt, position > 1)
print(sum(chrt$cohort.p * chrt$entropy))
scrs <- pblapply(1:5, function(i) {
d.scr <- shuffle_frequencies(d, shuffle_same_tail = T)
chrt.scr <- entropy_by_cohort(d.scr, include_post_up = T)
chrt.scr <- subset(chrt.scr, position > 1)
print(sum(chrt.scr$cohort.p * chrt.scr$entropy))
})
f <- 'Arabic.lex.txt'
d <- lf(paste0(lexdir, f))
f <- sub('.lex.txt', '', f)
print(f)
chrt <- entropy_by_cohort(d, include_post_up = F)
chrt <- subset(chrt, position > 1)
H.L <- sum(chrt$cohort.p * chrt$entropy)
scrs <- pbsapply(1:5, function(i) {
d.scr <- shuffle_frequencies(d, shuffle_same_tail = T)
chrt.scr <- entropy_by_cohort(d.scr, include_post_up = T)
chrt.scr <- subset(chrt.scr, position > 1)
sum(chrt.scr$cohort.p * chrt.scr$entropy)
})
print(sum(H.L < scrs) / 5)
#####
df <- lapply(lex.files, function(f) {
d <- lf(paste0(lexdir, f))
f <- sub('.lex.txt', '', f)
print(f)
chrt <- entropy_by_cohort(d, include_post_up = F)
chrt <- subset(chrt, position > 1)
H.L <- sum(chrt$cohort.p * chrt$entropy)
scrs <- pbsapply(1:5, function(i) {
d.scr <- shuffle_frequencies(d, shuffle_same_tail = T)
chrt.scr <- entropy_by_cohort(d.scr, include_post_up = T)
chrt.scr <- subset(chrt.scr, position > 1)
sum(chrt.scr$cohort.p * chrt.scr$entropy)
}, cl = nb_cores)
print(sum(H.L < scrs) / 5)
rbind(data.frame(language = f, lex.type = 'og', H = H.L, i = 1),
data.frame(language = f, lex.type = 'scr', H = scrs, i = 2:6))
})
